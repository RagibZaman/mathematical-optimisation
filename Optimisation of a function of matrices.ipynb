{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation of a function of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "Setting up python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.optimize as opt\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to minimize the following cost function $ f(X) $ defined\n",
    "over the space of real $ n \\times p $ matrices\n",
    "\\begin{equation}\n",
    "  f(X) = \\frac{1}{2} \\trace{X^T C X N} + \\mu \\frac{1}{4} \\Norm{N - X^T X}^2_F\n",
    "\\end{equation}\n",
    "where $ X \\in \\RR^{n \\times p} $, $ n \\ge p $, and the matrix $ C $ is symmetric, \n",
    "such that $ C = C^T $. The scalar $ \\mu $ is assumed to be larger than the $p$th smallest \n",
    "eigenvalue of $ C $. The matrix $ N $ is diagonal with distinct positive entries\n",
    "on the diagonal.\n",
    "The trace of a square matrix $ A $ is denoted as $ \\trace{A} $, and\n",
    "the Frobenius norm of an arbitrary matrix $ A $ is defined as $ \\Norm{A}_F = \\sqrt{\\trace{A^T A}} $. After some numerical experimentation we eventually compute a closed form of an optimal $X.$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenious Norm\n",
    "\n",
    "Implement a Python function ```frobenius_norm``` which accepts an arbitrary matrix $ A $ and returns\n",
    "$ \\Norm{A}_F $ using the formula given. (Use ```numpy.trace``` and ```numpy.sqrt```.)\n",
    "1. Given a matrix $ A \\in \\RR^{n \\times p} $, what is the complexity of your implementation of ```frobenius_norm```\n",
    "using the formula above?\n",
    "2. Can you come up with a faster implementation, if you were additionally told that $ p \\ge n $ ?\n",
    "3. Can you find an even faster implementation than in 1. and 2.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobenius_norm(A):\n",
    "    return np.sqrt(np.trace(np.dot(np.transpose(A), A)))\n",
    "\n",
    "# 1. Complexity is O(p**2 * n)\n",
    "\n",
    "# 2. Trace of a product is invariant under cyclic permutations. \n",
    "\n",
    "def frobenius_norm_fast(A):\n",
    "    return np.sqrt(np.trace(np.dot(A, np.transpose(A))))\n",
    "\n",
    "# 3. Sum the squares of the elements of A, which is O(n*p). That's what the native np norm does.\n",
    "\n",
    "def frobenius_norm_best(A):\n",
    "    return np.linalg.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function $f(X)$ with matrix argument\n",
    "\n",
    "Implement the cost function defined as $f(X)$ above as a function ```cost_function_for_matrix```\n",
    "in Python.\n",
    "\n",
    "Hint: As good programmers, we do not use global variables in subroutines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_for_matrix(X, C, N, u):\n",
    "    trace_term = 0.5 * np.trace( np.transpose(X) @ C @ X @ N )\n",
    "    norm_term = 0.25 * u * np.linalg.norm( N - np.transpose(X)@ X )**2\n",
    "    \n",
    "    return trace_term + norm_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function $f(X)$ with vector argument\n",
    "\n",
    "Many standard optimisation routines work only with vectors. Fortunately, as vector spaces,\n",
    "the space of matrices $ \\RR^{n \\times p} $ \n",
    "and the space of vectors $ \\RR^{n p} $ are isomorphic. What does this mean?\n",
    "\n",
    "Implement the cost function $ f(X) $ given $ X $ as a vector and call it ```cost_function_for_vector```.\n",
    "Which extra arguments do you need for the cost function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_for_vector(X, C, N, u, n, p):\n",
    "    X_mat = np.array(X).reshape(n,p)\n",
    "    \n",
    "    return cost_function_for_matrix(X_mat, C, N, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of a random matrix $C$ with given eigenvalues\n",
    "\n",
    "A diagonal matrix has the nice property that the eigenvalues can be directly read off\n",
    "the diagonal. Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues, \n",
    "how many different diagonal matrices have the same set of eigenvalues?\n",
    "\n",
    "Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues,\n",
    "how many different matrices have the same set of eigenvalues?\n",
    "\n",
    "Given a set of $ n $ distinct real eigenvalues $ \\mathcal{E} = \\{e_1, \\dots, e_n \\} $, \n",
    "write a Python function ```random_matrix_from_eigenvalues``` which takes a list of\n",
    "eigenvalues $ E $ and returns a random symmetric matrix $ C $ having the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_matrix_from_eigenvalues(E):\n",
    "    O = stats.ortho_group.rvs(dim=len(E)) # Random element of O(n)\n",
    "    return O @ np.diag(E) @ np.transpose(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the cost function $f(X)$\n",
    "\n",
    "Use the minimisation functions ```fmin``` or ```fmin_powell``` provided in the\n",
    "Python package ```scipy.optimize``` to minimise the cost function ```cost_function_for_vector```.\n",
    "\n",
    "Hint: Use the argument ```args``` in the minimisation functions  ```fmin``` or ```fmin_powell``` \n",
    "to provide the extra parameters to\n",
    "your cost function ```cost_function_for_vector```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_of_cost(cost_function_for_vector, C, N, u, n, p):\n",
    "\n",
    "    argmin_vector = opt.fmin_powell(cost_function_for_vector, np.zeros(n*p), args = (C, N, u, n, p), xtol = 10**-6)\n",
    "    return argmin_vector.reshape(n,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of $f(X)$\n",
    "\n",
    "Calculate the gradient for the cost function $f(X)$ given the\n",
    "inner product on the space of real matrices $ n \\times p $ is defined as\n",
    "\\begin{equation}\n",
    "  \\inner{A}{B} = \\trace{A^T B}\n",
    "\\end{equation}\n",
    "\n",
    "Implement a function ```gradient_for_vector``` which takes $ X $ as a vector, and\n",
    "returns the gradient of $ f(X) $ as a vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_for_vector(X, C, N, u, n, p):\n",
    "    X_mat = np.array(X).reshape(n,p)\n",
    "    \n",
    "    trace_term = C @ X_mat @ N\n",
    "    norm_term_1 = u * X_mat @ np.transpose(X_mat) @ X_mat\n",
    "    norm_term_2 = u * X_mat @ N\n",
    "    \n",
    "    gradient_mat = trace_term + norm_term_1 - norm_term_2\n",
    "    \n",
    "    return np.reshape(gradient_mat, n*p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the cost function $f(X)$ using the gradient\n",
    "\n",
    "Use the minimisation functions ```fmin_cg``` or ```fmin_bfgs``` provided in the\n",
    "Python package ```scipy.optimize``` to minimise the cost function ```cost_function_for_vector``` utilising the gradient ```gradient_for_vector```.\n",
    "\n",
    "Compare the speed of convergence to the minimisation with ```fmin``` or ```fmin_powell```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_of_cost_2(cost_function_for_vector, C, N, u, n, p):\n",
    "    \n",
    "    arg_min_vector= opt.fmin_bfgs(cost_function_for_vector, np.ones(n*p), gradient_for_vector, args = (C, N, u, n, p), gtol=10**-4)\n",
    "    return arg_min_vector.reshape(n,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minima of $f(X)$\n",
    "\n",
    "Compare the columns $x_1,\\dots, x_p$ of the matrix $X^\\star$ which minimises $ f(X) $ \n",
    "\\begin{equation}\n",
    "  X^\\star = \\argmin_{X \\in \\RR^{n \\times p}} f(X)\n",
    "\\end{equation}\n",
    "\n",
    "with the eigenvectors related to the smallest eigenvalues of $ C $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our numerical experiments below, we observe that for $i=1, \\cdots, p$, the column $x_{p-i+1}$ is an eigenvector corresponding to $\\lambda_i$, where $\\lambda_1, \\lambda_2, \\cdots, \\lambda_n$ is the sequence of eigenvalues of $C$ in ascending order.\n",
    "\n",
    "$C$ is a real symmetric matrix so it can be diagonalized: $C = P D P^{-1}$ where $P$ is a matrix whose $i$-th column is an eigenvector corresponding to the $i$-th eigenvalue $\\lambda_i$.\n",
    "Further, we can assume that $P$ is an orthogonal matrix : $P^T P = P P^T = I$, i.e. the columns of P are an an orthonormal basis of $\\mathbb{R}^n$.\n",
    "\n",
    "We therefore have the conjecture that $$X^* = [b_1 P_p \\ | \\ b_2 P_{p-1} \\ | \\ \\ldots \\ | \\ b_p P_1]\n",
    "= [P_p \\ | \\ P_{p-1} \\ | \\ \\ldots \\ | \\ P_1] \\ \\operatorname{diag}(b_1, b_2, \\ldots , b_p)$$ for some $b_1, \\cdots, b_p \\in \\mathbb{R}$. To find $b_i$, set $$\\frac{df}{dX} = CXN + \\mu ( X X^T X - XN)$$ to be equal to $0$. \n",
    "Some calculation shows $$ \\left.\\frac{df}{dX}\\right|_{X= X^*} =\n",
    "[P_p \\ | \\ P_{p-1} \\ | \\ \\ldots \\ | \\ P_1] * \\operatorname{diag}(\\ \\mu b_1^3 + b_1(\\lambda_p n_1 - \\mu n_1) \\ , \\ldots , \\  \\mu b_p^3 + b_p(\\lambda_1 n_p - \\mu n_p ) \\ )$$\n",
    "\n",
    "Assume further that $\\mu > \\operatorname{max}(0, \\lambda_p)$. Then $b_i = \\sqrt{ n_i \\left( 1 - \\dfrac{\\lambda_{p-i-1}}{\\mu} \\right)}$ provides a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_of_f(C, N, u, n, p):\n",
    "    '''\n",
    "    Closed form solution for the argmin using the above.\n",
    "    '''\n",
    "    vals, vecs = np.linalg.eig(C)\n",
    "    idx = vals.argsort()[:]\n",
    "    vals = vals[idx]\n",
    "    vecs = vecs[:,idx]\n",
    "\n",
    "    b = [ (N[i,i]/u *(u - vals[p - 1 - i]))**.5 for i in range(p) ]\n",
    "    X_argmin = np.empty([n,p])\n",
    "\n",
    "    for i in range(p):\n",
    "        X_argmin[:,i] = b[i] * vecs[:,p - 1 - i]\n",
    "    return X_argmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Experiments\n",
    "\n",
    "Here we use our numerical solver and observe the relation between the columns on $X^*$ (evaluated numerically) and the eigenvalues/eigenvectors of $C,$ which lead to our conjecture for $X^*$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ = 6\n",
    "p_ = 4\n",
    "C_ = random_matrix_from_eigenvalues([-3, 3 , 7.4 , 5.7, -2.4, -0.3])\n",
    "u_ = 9\n",
    "N_ = np.diag([1,3,4,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.8825"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = argmin_of_f(C_, N_, u_, n_, p_)\n",
    "#print(r)\n",
    "cost_function_for_matrix(r, C_, N_, u_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -39.592531\n",
      "         Iterations: 34\n",
      "         Function evaluations: 10335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-39.592531115826219"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = argmin_of_cost(cost_function_for_vector, C_, N_, u_, n_, p_)\n",
    "#print(s)\n",
    "cost_function_for_matrix(s, C_, N_, u_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -39.600000\n",
      "         Iterations: 85\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-39.599999999798314"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = argmin_of_cost_2(cost_function_for_vector, C_, N_, u_, n_, p_)\n",
    "#print(t)\n",
    "cost_function_for_matrix(t, C_, N_, u_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.  -2.4 -0.3  3.   5.7  7.4]\n",
      "[[ 0.39177197  0.35075259 -0.78666426  0.05855447  0.12398878  0.2929929 ]\n",
      " [-0.05317198 -0.4041596  -0.29244146 -0.13476249  0.69482558 -0.49735524]\n",
      " [-0.41329929 -0.07999078  0.05139486  0.64516578  0.41385018  0.48232032]\n",
      " [-0.32267527 -0.58491359 -0.49563168  0.10160758 -0.54557239  0.0115178 ]\n",
      " [-0.36177385 -0.04921734 -0.03185795 -0.74286385  0.16860421  0.53423621]\n",
      " [-0.6617186   0.60223156 -0.21524454 -0.00087425 -0.06704932 -0.38551141]]\n"
     ]
    }
   ],
   "source": [
    "# Eigenvalues (increasing) of C and corresponding eigenvector (in particular, the unit length one)\n",
    "vals, vecs = np.linalg.eig(C_)\n",
    "idx = vals.argsort()[:]\n",
    "vals = vals[idx]\n",
    "vecs = vecs[:,idx]\n",
    "print(vals)\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -39.600000\n",
      "         Iterations: 85\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 98\n",
      "[[  4.78087538e-02   6.83745606e-01  -9.04756754e-01   1.13090103e+00]\n",
      " [ -1.10030710e-01  -7.87852137e-01   1.22793548e-01   4.20412119e-01]\n",
      " [  5.26777847e-01  -1.55932990e-01   9.54474247e-01  -7.38846169e-02]\n",
      " [  8.29600865e-02  -1.14020735e+00   7.45182861e-01   7.12517092e-01]\n",
      " [ -6.06546036e-01  -9.59454326e-02   8.35480747e-01   4.57987625e-02]\n",
      " [ -7.14495716e-04   1.17396321e+00   1.52817812e+00   3.09433490e-01]]\n"
     ]
    }
   ],
   "source": [
    "X_a = argmin_of_cost_2(cost_function_for_vector, C_, N_, u_, n_, p_)\n",
    "print(X_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -66.91258282  153.99771862 -737.27222612 -116.10998447  848.91486703\n",
      "    1.        ]\n",
      "\n",
      "[ 0.58242507 -0.67110462 -0.13282613 -0.97124624 -0.0817278   1.        ]\n",
      "\n",
      "[-0.59204928  0.0803529   0.62458311  0.48762827  0.54671686  1.        ]\n",
      "\n",
      "[ 3.6547467   1.35865099 -0.23877382  2.30265021  0.14800842  1.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(p_):\n",
    "    print(X_a[:,i]/X_a[:,i][-1])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.59205223  0.08035437  0.62458465  0.48763216  0.54671858  1.        ]\n",
      "\n",
      "[ 0.58242147 -0.67110332 -0.13282395 -0.97124367 -0.08172493  1.        ]\n",
      "\n",
      "[ 3.65474669  1.35864754 -0.23877429  2.30264467  0.14800817  1.        ]\n",
      "\n",
      "[ -66.97715079  154.14721173 -737.9687541  -116.22318068  849.7200684     1.        ]\n",
      "\n",
      "[ -1.84921756 -10.36290236  -6.1723246    8.13688153  -2.51462963   1.        ]\n",
      "\n",
      "[-0.76001098  1.29011809 -1.25111815 -0.02987668 -1.38578572  1.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_):\n",
    "    print(vecs[:,i]/vecs[:,i][-1])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "123: 321,\n",
    "132: 312,\n",
    "213: 231,\n",
    "231: 213,\n",
    "312: 132,\n",
    "321: 123,\n",
    "What is this mapping? 4-complement\n",
    "\n",
    "12: 21,\n",
    "21: 12,\n",
    "3-complement\n",
    "\n",
    "1234 : 4321,\n",
    "1243 : 4312,\n",
    "1342 : 4213,\n",
    "5-complement\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
