# Optimization of Bregman Divergences
 An essay about Bregman Divergences, discussing how they arise naturally in machine learning and statistics. Supplementary to the essay is a notebook implementing and testing some of the algorithms discussed in the essay. 

## Abstract 

In this report we survey Bregman Divergences and motivate their study by forming connections to areas such as convex optimization, machine learning and statistics. We give particular focus to developing an understanding of the optimization of Bregman Divergences and the connection to Exponential Families and Logistic Regression. In the second chapter we see that the Kullback-Liebler divergence between two members of an exponential family can be expressed as a Bregman Divergence between the distribution parameters. The third chapter gives a brief overview of Generalized Linear Models and Logistic Regression. We give a moderately simplified proof of a theorem of A.~ Banerjee which states that the log-odds ratio of the class posteriors is affine (which is the modelling assumption of Logistic Regression) if and only if the class conditional distributions belong to a fixed natural exponential family. In the final chapter, we show how the problem of finding the optimal parameters for the log loss of Logistic Regression can be cast as a Bregman Divergence optimization problem, and prove the convergence of an algorithm for finding those parameters using methods standard in the optimization of Bregman divergences. By simplifying their assumptions and/or proofs where we were able to, we believe we have given a clearer presentation of the results which we have drawn from our references. We hope that the reader will come away with a keen interest in Bregman Divergences and be aware of potential applications in their fields of study.