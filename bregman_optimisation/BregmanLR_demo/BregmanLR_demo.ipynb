{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Logistic Regression Algorithms\n",
    "\n",
    "In this notebook, we implement two algorithms for Logistic Regression inspired by Bregman distances presented in 'Logistic Regression, AdaBoost and Bregman Distances' (Schapire et al 2002) and 'Bregman Distance to L1 Regularized Logistic Regression' (Huang and Gupta, 2010). We compare them to two more standard algorithms - Logistic Regression (LBFGS, No Regularization) and Lasso Regression (via Coordinate Descent). \n",
    "\n",
    "We run these 4 algorithms to perform binary classification on a variety of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "from sklearn import preprocessing, datasets, model_selection, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "Data is processed so labels are +1,-1 (which is useful for the algorithms we implement). We split off 1/7-th of the data as the test set and use the rest for training. For datasets with more than 2 classes (in particular, MNIST and Fashion MNIST), we filter the instances for those belonging to 2 particular classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary of datasets, keys are names (string) of datasets, values are 4 tuples: \n",
    "# (X_train, X_test, y_train, y_test)\n",
    "data_dict = dict()\n",
    "lb = preprocessing.LabelBinarizer(neg_label=-1, pos_label = 1)\n",
    "\n",
    "for openml_id in [554, 40996, 59, 37, 53, 1510]:\n",
    "    dataset = datasets.fetch_openml(data_id = openml_id)\n",
    "    good_targets = np.unique(dataset.target)[0:2]\n",
    "    ix = np.isin(dataset.target, good_targets)\n",
    "    X = dataset.data[ix]\n",
    "    y = lb.fit_transform(dataset.target[ix]).ravel()\n",
    "    data_dict[dataset.details['name']]= model_selection.train_test_split(X, y, test_size=1/7.0, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (No Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogitR(X_train, X_test, y_train, y_test):\n",
    "    # sklearn's Logistic Regression has no option to disable regularization\n",
    "    # We set the regularization strength very low to get similar results\n",
    "    Logit_model = linear_model.LogisticRegression(C = 1e9, solver = 'lbfgs', max_iter = 1e4)\n",
    "    Logit_model.fit(X_train, y_train)\n",
    "    accuracy = Logit_model.score(X_test, y_test)\n",
    "    \n",
    "    # The predictions and weights can also be calculated:\n",
    "    #predictions = Logit_model.predict(X_test)\n",
    "    #weights = np.concatenate([Logit_model.intercept_, Logit_model.coef_[0]])\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lasso Regression (L1 Regularization)\n",
    "\n",
    "The regularization strength is determined by Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LassoR(X_train, X_test, y_train, y_test):\n",
    "    Lasso_model = linear_model.LassoCV(cv=4)\n",
    "    Lasso_model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = np.sign(Lasso_model.predict(X_test))\n",
    "    accuracy = np.mean(predictions==y_test)\n",
    "    #weights = np.concatenate([[Lasso_model.intercept_], Lasso_model.coef_])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bregman Logistic Regression by Schapire et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BregmanLogit(X_train, X_test, y_train, y_test, max_iters = 1000):\n",
    "    # First preprocess the data to \n",
    "    # 1) Include a bias parameter \n",
    "    # 2) Scale all instances x_i so the max l1 norm is = 1/2\n",
    "    X_train = np.concatenate([np.ones((X_train.shape[0],1)), X_train], axis = 1)\n",
    "    X_test = np.concatenate([np.ones((X_test.shape[0],1)), X_test], axis = 1)\n",
    "    \n",
    "    l1_max = max(np.linalg.norm(X_train, ord = np.inf), np.linalg.norm(X_test, ord = np.inf))\n",
    "    X_train = X_train/(2*l1_max)\n",
    "    X_test = X_test/(2*l1_max)\n",
    "    \n",
    "    n_train_samples, x_dim = X_train.shape\n",
    "    \n",
    "    \n",
    "    # Train weight vector (Parallel Algorithm, Section 5)\n",
    "    w = np.zeros(x_dim)\n",
    "    q = 1/2 * np.ones(n_train_samples)\n",
    "    M = X_train * y_train[:, np.newaxis] # Makes M[i] = y[i] * x[i] so M[i][j] = y[i] x[i][j]\n",
    "    \n",
    "    M_pos = np.multiply(M, M>0)\n",
    "    M_neg = np.multiply(-M, M<0)\n",
    "\n",
    "    for t in range(1,max_iters+1):\n",
    "        # Update q\n",
    "        if t==1: \n",
    "            q = 1/2 * np.ones(n_train_samples)\n",
    "        if t>1: \n",
    "            q = np.divide(q, np.multiply(1-q, np.exp(M @ d)) + q)\n",
    "        \n",
    "        # Update d\n",
    "        W_pos = q @ M_pos\n",
    "        W_neg = q @ M_neg\n",
    "\n",
    "        def delta(w_pos, w_neg):\n",
    "            # delta is picked to minimize the summand in Equation 27\n",
    "            if w_pos == 0 and w_neg == 0:\n",
    "                return 0\n",
    "            if w_pos == 0 and w_neg != 0:\n",
    "                return -99\n",
    "            if w_pos != 0 and w_neg == 0:\n",
    "                return 99\n",
    "            return 1/2 * np.log(w_pos/w_neg)\n",
    "\n",
    "        delta_vec = np.vectorize(delta)\n",
    "        d = delta_vec(W_pos, W_neg)\n",
    "        w += d\n",
    "\n",
    "        if t>1 and abs(np.linalg.norm(w, ord=1)/np.linalg.norm(w-d, ord=1)-1) < 1e-4:\n",
    "            # print(\"Converged on iter:\", t)\n",
    "            break\n",
    "            \n",
    "    # Make predictions on test and evaluate accuracy\n",
    "    # Pass X_test @ w through h for class probablities\n",
    "    # from scipy.special import expit as h # Logistic Sigmoid\n",
    "    predictions = np.sign(X_test @ w)\n",
    "    accuracy = np.mean(y_test.T==predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bregman Logistic Regression with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BregmanLogit_Reg(X_train, X_test, y_train, y_test, alpha, max_iters = 1000):\n",
    "    # First preprocess the data to \n",
    "    # 1) Include a bias parameter \n",
    "    # 2) Scale all instances x_i so the max l1 norm is = 1/2\n",
    "    X_train = np.concatenate([np.ones((X_train.shape[0],1)), X_train], axis = 1)\n",
    "    X_test = np.concatenate([np.ones((X_test.shape[0],1)), X_test], axis = 1)\n",
    "    \n",
    "    l1_max = max(np.linalg.norm(X_train, ord = np.inf), np.linalg.norm(X_test, ord = np.inf))\n",
    "    X_train = X_train/(2*l1_max)\n",
    "    X_test = X_test/(2*l1_max)\n",
    "    \n",
    "    n_train_samples, x_dim = X_train.shape\n",
    "    \n",
    "    \n",
    "    # Train weight vector (Parallel Algorithm, Section 5)\n",
    "    w = np.zeros(x_dim)\n",
    "    q = 1/2 * np.ones(n_train_samples)\n",
    "    M = X_train * y_train[:, np.newaxis] # Makes M[i] = y[i] * x[i] so M[i][j] = y[i] x[i][j]\n",
    "    \n",
    "    M_pos = np.multiply(M, M>0)\n",
    "    M_neg = np.multiply(-M, M<0)\n",
    "    \n",
    "    for t in range(1,max_iters+1):\n",
    "        # Update q\n",
    "        if t==1: \n",
    "            q = 1/2 * np.ones(n_train_samples)\n",
    "        if t>1: \n",
    "            q = np.divide(q, np.multiply(1-q, np.exp(M @ d)) + q)\n",
    "        \n",
    "        # Update d\n",
    "        W_pos = q @ M_pos\n",
    "        W_neg = q @ M_neg\n",
    "        \n",
    "        d = cvx.Variable(x_dim)\n",
    "        # d is chosen to minimize Equation 27 (Schapire et al)\n",
    "        bregman_bound = W_pos * (cvx.exp(-d) - 1) + W_neg * (cvx.exp(d)-1)\n",
    "        objective = cvx.Minimize(bregman_bound)\n",
    "        constraint = [cvx.norm1(w+d) <= alpha]\n",
    "        prob = cvx.Problem(objective, constraint)\n",
    "        prob.solve()  # Returns the optimal value.\n",
    "        d = d.value\n",
    "        w += d\n",
    "        \n",
    "        if t>1 and abs(np.linalg.norm(w, ord=1)/np.linalg.norm(w-d, ord=1)-1) < 1e-3:\n",
    "            #print(\"Converged on iter:\", t)\n",
    "            break\n",
    "            \n",
    "    # Make predictions on test and evaluate accuracy\n",
    "    # Pass X_test @ w through h for class probablities\n",
    "    # from scipy.special import expit as h # Logistic Sigmoid\n",
    "    predictions = np.sign(X_test @ w)\n",
    "    accuracy = np.mean(y_test.T==predictions)\n",
    "    return w, predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogitR\n",
      "\n",
      "mnist_784 0.9976325757575758\n",
      "Fashion-MNIST 0.984\n",
      "ionosphere 0.9803921568627451\n",
      "diabetes 0.8272727272727273\n",
      "heart-statlog 0.8974358974358975\n",
      "wdbc 0.9878048780487805\n",
      "--------------------\n",
      "\n",
      "LassoR\n",
      "\n",
      "mnist_784 0.9910037878787878\n",
      "Fashion-MNIST 0.989\n",
      "ionosphere 0.9019607843137255\n",
      "diabetes 0.8363636363636363\n",
      "heart-statlog 0.8717948717948718\n",
      "wdbc 0.9512195121951219\n",
      "--------------------\n",
      "\n",
      "BregmanLogit\n",
      "\n",
      "mnist_784 0.9962121212121212\n",
      "Fashion-MNIST 0.988\n",
      "ionosphere 0.8823529411764706\n",
      "diabetes 0.7363636363636363\n",
      "heart-statlog 0.8717948717948718\n",
      "wdbc 0.9024390243902439\n",
      "--------------------\n",
      "\n",
      "BregmanLogit_Reg\n",
      "\n",
      "mnist_784 0.9990530303030303\n",
      "Fashion-MNIST 0.9695\n",
      "ionosphere 0.8627450980392157\n",
      "diabetes 0.7272727272727273\n",
      "heart-statlog 0.8974358974358975\n",
      "wdbc 0.8658536585365854\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracies of the 4 algorithms on the datasets\n",
    "\n",
    "for func in [LogitR, LassoR, BregmanLogit]:\n",
    "    print(func.__name__ + '\\n')\n",
    "    for key, value in data_dict.items():\n",
    "        print(key, func(*value))\n",
    "    print('--------------------\\n')\n",
    "    \n",
    "print(\"BregmanLogit_Reg\\n\")    \n",
    "for key, value in data_dict.items():\n",
    "    if key in ['mnist_784', 'Fashion-MNIST']:\n",
    "        # We limit the number of iterations for these cases because\n",
    "        # cvxpy has convergences issues on these high dimensional datasets\n",
    "        print(key, BregmanLogit_Reg(*value, 1e5, 100)[2])\n",
    "    else:\n",
    "        print(key, BregmanLogit_Reg(*value, 1e3)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "|               |Instances | Features| LR        | Bregman LR           | LR+L1  |Bregman LR+L1\n",
    "| ------------- |:-------------:| -----:|                |         |\n",
    "| MNIST      | 14780 | 784 | 99.8             | 99.6 |     99.1        |     99.9      |\n",
    "| Fashion MNIST|  14000 | 784 |   98.4  |   98.8 |     98.9      |     97.0          |\n",
    "| Ionosphere      | 351 | 35 | 98.0      |  88.2   |    90.2       |    86.3           |\n",
    "| Diabetes      | 768 | 8 |   82.7   |   73.7  |          83.6 |     72.7          |\n",
    "| Heart Statlog      |  270 | 13 |  89.7  |  87.2   |          87.2 |   89.7            |\n",
    "| WDBC      | 569 | 30 |   98.8   |   90.2  |          95.1 |      86.6         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no obvious relation between the number of features/instances vs the relative performance of the Bregman versions over the non-Bregman versions.\n",
    "\n",
    "On the high dimensional data (MNIST and Fasion MNIST) all 4 algorithms perform very well and quite similarly. In the other datasets however our implementations of the Bregman versions perform consistently worse than the standard versions, with up to a 10% decrease in prediction accuracy (as seen with the unregularized Ionosphere results).\n",
    "\n",
    "It is difficult to completely understand where these differences have come from, as we could not always set all analogous choices to be the same in the algorithms so that we could more directly compare them. For example, the regularization in the Lasso (LR+L1) model is the Tikonov variant, whereas the Bregman LR+L1 algorithm performs Ivanov regularization. Further, this Ivanov regularization parameter in the Bregman LR+L1 algorithm, as well as the number of iterations, could not be set completely freely - the optimization package CVXPY runs into convergences issues if these are altered to bad values. For the Bregman LR algorithm, we encountered an edge case not explicitly mentioned in the paper by Collins, Schapire and Singer where certain weights may need to be adjusted by -inf or +inf. At first we tried simply add or substract a large number in these cases, however if the number of iterations became too large these weights became np.nan and this would flow through our results. Thus, we were somewhat artifically limited in the number of iterations we could perform by this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
